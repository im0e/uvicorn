---
name: Performance Tests

on:
  # Run on PRs that modify code or benchmarks
  pull_request:
    paths:
      - 'uvicorn/**'
      - 'tests/benchmarks/**'
      - 'scripts/run_performance_tests.py'
      - '.github/workflows/performance.yml'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      quick_mode:
        description: 'Run quick tests (faster)'
        required: false
        default: 'true'
        type: boolean

jobs:
  performance:
    name: "Performance Benchmarks"
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install httpx psutil

      - name: Run performance tests (quick mode)
        if: github.event_name == 'pull_request' || inputs.quick_mode == 'true'
        run: |
          python scripts/run_performance_tests.py \
            --quick \
            --output performance_report.txt

      - name: Run performance tests (full mode)
        if: github.event_name == 'workflow_dispatch' && inputs.quick_mode == 'false'
        run: |
          python scripts/run_performance_tests.py \
            --output performance_report.txt

      - name: Display performance summary
        if: always()
        run: |
          if [ -f performance_report.txt ]; then
            echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat performance_report.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: |
            performance_report.txt
            performance_report.json
          retention-days: 30

      - name: Check for performance regression
        if: github.event_name == 'pull_request'
        run: |
          # Simple regression check - ensure tests completed successfully
          if [ -f performance_report.json ]; then
            echo "‚úÖ Performance tests completed successfully"
            
            # Extract key metrics using python
            python << 'EOF'
          import json
          import sys
          
          with open('performance_report.json') as f:
              data = json.load(f)
          
          benchmarks = data.get('tests', {}).get('simple_benchmark', {})
          
          if not benchmarks:
              print("‚ö†Ô∏è  No benchmark results found")
              sys.exit(1)
          
          print("\nüìä Performance Summary:")
          for key in sorted(benchmarks.keys()):
              result = benchmarks[key]
              print(f"  Concurrency {result['concurrency']:>3}: "
                    f"{result['rps']:>8.0f} req/s | "
                    f"p50: {result['p50_ms']:>6.2f}ms | "
                    f"p99: {result['p99_ms']:>6.2f}ms")
          
          # Basic sanity checks
          for key, result in benchmarks.items():
              if result['rps'] < 100:
                  print(f"\n‚ö†Ô∏è  Warning: Low throughput at concurrency {result['concurrency']}")
              if result['p99_ms'] > 1000:
                  print(f"\n‚ö†Ô∏è  Warning: High p99 latency at concurrency {result['concurrency']}")
          
          print("\n‚úÖ Performance metrics look reasonable")
          EOF
          else
            echo "‚ùå Performance report not found"
            exit 1
          fi

  comment-pr:
    name: "Comment Performance Results"
    runs-on: ubuntu-latest
    needs: performance
    if: github.event_name == 'pull_request' && always()
    permissions:
      pull-requests: write

    steps:
      - name: Download performance report
        uses: actions/download-artifact@v4
        with:
          name: performance-report

      - name: Comment PR with results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let reportText = '';
            try {
              reportText = fs.readFileSync('performance_report.txt', 'utf8');
            } catch (e) {
              reportText = '‚ùå Performance test failed or report not found';
            }
            
            const comment = `## üöÄ Performance Test Results
            
            <details>
            <summary>Click to expand performance report</summary>
            
            \`\`\`
            ${reportText}
            \`\`\`
            
            </details>
            
            ---
            *This is an automated performance test for this uvicorn fork. See [PERFORMANCE_TESTING.md](../blob/main/PERFORMANCE_TESTING.md) for more information.*
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
